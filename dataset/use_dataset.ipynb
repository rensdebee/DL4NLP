{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sample of how to use the dataset, which was used for internal illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Not sure if you have to login and provide a token made via your huggingface account -> settings -> acces token \n",
    "# # Or if it's fine as it's a public repo otherwise just skip this for now!\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4476442e574339b17312be5c9070d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb626916bef4a019adb28f79a1a0238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/12.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde70879e2ae4786a8ae29e31f44f3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "non_reddit_test-00000-of-00001.parquet:   0%|          | 0.00/1.54M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89edfcbe4c5f43b38152d6edb2c668be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff535417cbe240f6803861240572afee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bb7570dff14d60b7929a2ed89ed0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating reddit_train split:   0%|          | 0/5200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94b0c06f0164f5096b7a0deb25e7a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating reddit_test split:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e81a54a4583476e8996227455c3874e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating wiki_csai split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31c97f9523647069476405b686785bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating open_qa split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5d9753c7954d1a9ba9d87682aa6eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating finance split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13553fd87384abfa5d4a27b49f080bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating medicine split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb231ab326b04b2fb483fd867818d1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating non_reddit_test split:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "ds = load_dataset(\"DanteZD/HC3_plus_llama70B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'human_answers', 'chatgpt_answers', 'llama_answers', 'source'],\n",
       "        num_rows: 5200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'human_answers', 'chatgpt_answers', 'llama_answers', 'source'],\n",
       "        num_rows: 1600\n",
       "    })\n",
       "    reddit_train: Dataset({\n",
       "        features: ['id', 'question', 'human_answers', 'chatgpt_answers', 'llama_answers', 'source'],\n",
       "        num_rows: 5200\n",
       "    })\n",
       "    reddit_test: Dataset({\n",
       "        features: ['id', 'question', 'human_answers', 'chatgpt_answers', 'llama_answers', 'source'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    wiki_csai: Dataset({\n",
       "        features: ['id', 'question', 'human_answers', 'chatgpt_answers', 'llama_answers', 'source'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    open_qa: Dataset({\n",
       "        features: ['id', 'question', 'human_answers', 'chatgpt_answers', 'llama_answers', 'source'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    finance: Dataset({\n",
       "        features: ['id', 'question', 'human_answers', 'chatgpt_answers', 'llama_answers', 'source'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    medicine: Dataset({\n",
       "        features: ['id', 'question', 'human_answers', 'chatgpt_answers', 'llama_answers', 'source'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    non_reddit_test: Dataset({\n",
       "        features: ['id', 'question', 'human_answers', 'chatgpt_answers', 'llama_answers', 'source'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how the dataset currently looks:\n",
    "# train = 2200 reddit eli5 rows/entries\n",
    "# test = 800 reddit_eli5, and 200 wiki_csai, open_qa, finance, medicine rows (combined thus all sources are present)\n",
    "# reddit_train currently == train (but included this for maybe future)\n",
    "# reddit_test is only the 800 reddit_eli5 test rows (seperate)\n",
    "# the others (wiki_csai, open_qa, finance, medicine) are all the 200 test rows for their corresponding data sources (seperate)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of oading splits / subsets (thus only data of a seperate source)\n",
    "ds_train = load_dataset(\"DanteZD/HC3_plus_llama70B\", split=\"train\")\n",
    "ds_test = load_dataset(\"DanteZD/HC3_plus_llama70B\", split=\"test\")\n",
    "ds_reddit_test = load_dataset(\"DanteZD/HC3_plus_llama70B\", split=\"reddit_test\")\n",
    "ds_open_qa_test = load_dataset(\"DanteZD/HC3_plus_llama70B\", split=\"open_qa\")\n",
    "ds_wiki_csai_test = load_dataset(\"DanteZD/HC3_plus_llama70B\", split=\"wiki_csai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives you a dataset object (instead of a dataset dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'question': 'Why is every book I hear about a \" NY Times # 1 Best Seller \" ? ELI5 : Why is every book I hear about a \" NY Times # 1 Best Seller \" ? Should n\\'t there only be one \" # 1 \" best seller ? Please explain like I\\'m five.', 'human_answers': ['Basically there are many categories of \" Best Seller \" . Replace \" Best Seller \" by something like \" Oscars \" and every \" best seller \" book is basically an \" oscar - winning \" book . May not have won the \" Best film \" , but even if you won the best director or best script , you \\'re still an \" oscar - winning \" film . Same thing for best sellers . Also , IIRC the rankings change every week or something like that . Some you might not be best seller one week , but you may be the next week . I guess even if you do n\\'t stay there for long , you still achieved the status . Hence , # 1 best seller .', \"If you 're hearing about it , it 's because it was a very good or very well - publicized book ( or both ) , and almost every good or well - publicized book will be # 1 on the NY Times bestseller list for at least a little bit . Kindof like how almost every big or good movies are # 1 at the box office on their opening weekend .\", \"One reason is lots of catagories . However , how the NY Times calculates its best seller list is n't comprehensive , and is pretty well understood by publishers . So publishers can [ buy a few books ] ( URL_0 ) in the right bookstores and send a book to the top of the list for at least a week .\"], 'chatgpt_answers': ['There are many different best seller lists that are published by various organizations, and the New York Times is just one of them. The New York Times best seller list is a weekly list that ranks the best-selling books in the United States based on sales data from a number of different retailers. The list is published in the New York Times newspaper and is widely considered to be one of the most influential best seller lists in the book industry. \\nIt\\'s important to note that the New York Times best seller list is not the only best seller list out there, and there are many other lists that rank the top-selling books in different categories or in different countries. So it\\'s possible that a book could be a best seller on one list but not on another. \\nAdditionally, the term \"best seller\" is often used more broadly to refer to any book that is selling well, regardless of whether it is on a specific best seller list or not. So it\\'s possible that you may hear about a book being a \"best seller\" even if it is not specifically ranked as a number one best seller on the New York Times list or any other list.'], 'llama_answers': ['The New York Times Best Seller list is like a special chart that shows which books are selling the most copies. There are many different lists for different kinds of books, like fiction, non-fiction, and children\\'s books. So, a book can be #1 on one list, but not on another. Imagine you\\'re in a big library with many different rooms. Each room has its own \"best seller\" list. A book can be the best seller in one room (like the romance room), but not in another room (like the sci-fi room). That\\'s why you might hear about many different books being #1 best sellers. They\\'re just #1 in their own special room!'], 'source': 'reddit_eli5'}\n"
     ]
    }
   ],
   "source": [
    "# This gives you a dataset object (instead of a dataset dict) over which you can iterate and use to train / test\n",
    "for entry in ds_test:\n",
    "    print(entry)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'question', 'human_answers', 'chatgpt_answers', 'llama_answers', 'source'])\n",
      "where are colors on stoplight\n",
      "['The colors on a standard stoplight, also known as a traffic light, are typically arranged vertically in the following order from top to bottom:\\n\\n1. Red\\n2. Yellow\\n3. Green\\n\\nThis order is consistent in most countries, including the United States, Canada, and many others. The colors are positioned in this order for a few reasons:\\n\\n* Red is at the top because it is the most visible color from a distance, making it more noticeable to drivers.\\n* Yellow is in the middle because it is a transitional color that indicates caution and prepares drivers for the change from red to green.\\n* Green is at the bottom because it is the color that indicates go, and it is often associated with safety and movement.\\n\\nThis standard arrangement helps to ensure that drivers can easily understand the traffic signal and respond accordingly.']\n",
      "open_qa\n"
     ]
    }
   ],
   "source": [
    "# Or just index freely\n",
    "print(ds_test[1005].keys())\n",
    "print(ds_test[1005][\"question\"])\n",
    "print(ds_test[1005][\"llama_answers\"])\n",
    "print(ds_test[1005][\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more info see:\n",
    "# https://huggingface.co/docs/datasets/index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
